[
    {
        "type": "plugin",
        "name": "google",
        "org": "langgenius",
        "plugin_id": "langgenius/google",
        "icon": "langgenius/packages/google/_assets/icon.svg",
        "label": {
            "en_US": "Google"
        },
        "brief": {
            "en_US": "A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.",
            "zh_Hans": "一个用于执行 Google SERP 搜索并提取片段和网页的工具。输入应该是一个搜索查询。"
        },
        "introduction": "# Google Search\n\n## Overview\n\nThe Google Search tool is a real-time API that extracts search engine results, providing structured data from Google. It supports various search types, including web, image, news, and maps.\n\n## Configuration\n\n### 1. Apply for an API Key\n\nPlease apply for an API Key on the [SerpApi](https://serpapi.com/dashboard).\n\n![](./_assets/google_3.png)\n\n### 2. Get Google tools from Plugin Marketplace\n\nThe Google tools could be found at the Plugin Marketplace, please install it first.\n\n### 3. Fill in the configuration in Dify\n\nOn the Dify navigation page, click `Tools > Google > To authorize` filling in the API Key.\n\n![](./_assets/google_1.png)\n\n### 4. Use the tool\n\nYou can use the Google tool in the following application types:\n\n![](./_assets/google_2.png)\n\n#### Chatflow / Workflow applications\n\nBoth Chatflow and Workflow applications support adding a Google tool node.\n\n#### Agent applications\n\nAdd the Google tool in the Agent application, then enter online search instructions to call this tool.",
        "category": "tool",
        "repository": "",
        "resource": {
            "memory": 1048576,
            "permission": {
                "model": {
                    "enabled": true,
                    "llm": true,
                    "moderation": false,
                    "rerank": false,
                    "speech2text": false,
                    "text_embedding": false,
                    "tts": false
                },
                "tool": {
                    "enabled": true
                }
            }
        },
        "privacy_options": "",
        "privacy_policy": "",
        "created_at": "2024-12-05T13:13:45.167269Z",
        "updated_at": "2025-02-17T07:07:33.034344Z",
        "badges": [],
        "plugins": {
            "agent_strategies": null,
            "endpoints": null,
            "models": null,
            "tools": [
                "provider/google.yaml"
            ]
        },
        "endpoint": {},
        "model": {},
        "tool": {
            "credentials_schema": [
                {
                    "default": null,
                    "helper": null,
                    "label": {
                        "en_US": "SerpApi API key",
                        "pt_BR": "SerpApi API key",
                        "zh_Hans": "SerpApi API key"
                    },
                    "name": "serpapi_api_key",
                    "options": null,
                    "placeholder": {
                        "en_US": "Please input your SerpApi API key",
                        "pt_BR": "Please input your SerpApi API key",
                        "zh_Hans": "请输入你的 SerpApi API key"
                    },
                    "required": true,
                    "scope": null,
                    "type": "secret-input",
                    "url": "https://serpapi.com/manage-api-key"
                }
            ],
            "identity": {
                "author": "Dify",
                "description": {
                    "en_US": "Google",
                    "pt_BR": "Google",
                    "zh_Hans": "GoogleSearch"
                },
                "icon": "icon.svg",
                "label": {
                    "en_US": "Google",
                    "pt_BR": "Google",
                    "zh_Hans": "Google"
                },
                "name": "google",
                "tags": [
                    "search"
                ]
            },
            "tools": [
                {
                    "description": {
                        "human": {
                            "en_US": "A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.",
                            "pt_BR": "A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.",
                            "zh_Hans": "一个用于执行 Google SERP 搜索并提取片段和网页的工具。输入应该是一个搜索查询。"
                        },
                        "llm": "A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query."
                    },
                    "has_runtime_parameters": false,
                    "identity": {
                        "author": "Dify",
                        "label": {
                            "en_US": "GoogleSearch",
                            "pt_BR": "GoogleSearch",
                            "zh_Hans": "谷歌搜索"
                        },
                        "name": "google_search"
                    },
                    "output_schema": null,
                    "parameters": [
                        {
                            "auto_generate": null,
                            "default": null,
                            "form": "llm",
                            "human_description": {
                                "en_US": "used for searching",
                                "pt_BR": "used for searching",
                                "zh_Hans": "用于搜索网页内容"
                            },
                            "label": {
                                "en_US": "Query string",
                                "pt_BR": "Query string",
                                "zh_Hans": "查询语句"
                            },
                            "llm_description": "key words for searching",
                            "max": null,
                            "min": null,
                            "name": "query",
                            "options": null,
                            "precision": null,
                            "required": true,
                            "scope": null,
                            "template": null,
                            "type": "string"
                        }
                    ]
                }
            ]
        },
        "agent_strategy": {},
        "install_count": 8448,
        "version_updated_at": "2025-02-17T07:07:33.034312Z",
        "latest_version": "0.0.8",
        "latest_package_identifier": "langgenius/google:0.0.8@3efcf55ffeef9d0f77715e0afb23534952ae0cb385c051d0637e86d71199d1a6",
        "tags": [
            {
                "name": "search"
            }
        ]
    },
    {
        "type": "plugin",
        "name": "bedrock",
        "org": "langgenius",
        "plugin_id": "langgenius/bedrock",
        "icon": "langgenius/packages/bedrock/_assets/icon_s_en.svg",
        "label": {
            "en_US": "Amazon Bedrock",
            "ja_JP": "Amazon Bedrock",
            "pt_BR": "Amazon Bedrock",
            "zh_Hans": "Amazon Bedrock"
        },
        "brief": {
            "en_US": "The models of Amazon Bedrock",
            "ja_JP": "The models of Amazon Bedrock",
            "pt_BR": "The models of Amazon Bedrock",
            "zh_Hans": "The models of Amazon Bedrock"
        },
        "introduction": "## amazon-bedrock\n\n**Author:** aws\n**Version:** 0.0.1\n**Type:** extension\n\n### Description\n\n\n\n",
        "category": "model",
        "repository": "",
        "resource": {
            "memory": 268435456,
            "permission": {
                "model": {
                    "enabled": true,
                    "llm": true,
                    "moderation": false,
                    "rerank": true,
                    "speech2text": false,
                    "text_embedding": true,
                    "tts": false
                }
            }
        },
        "privacy_options": "",
        "privacy_policy": "PRIVACY.md",
        "created_at": "2025-01-02T08:10:48.514502Z",
        "updated_at": "2025-03-14T14:52:44.902833Z",
        "badges": [],
        "plugins": {
            "agent_strategies": null,
            "endpoints": null,
            "models": [
                "provider/amazon-bedrock.yaml"
            ],
            "tools": null
        },
        "endpoint": {},
        "model": {
            "background": "#FCFDFF",
            "configurate_methods": [
                "predefined-model"
            ],
            "description": {
                "en_US": "Bedrock LLM Model"
            },
            "help": {
                "title": {
                    "en_US": "Get your Access Key and Secret Access Key from AWS Console"
                },
                "url": {
                    "en_US": "https://console.aws.amazon.com/"
                }
            },
            "icon_large": {
                "en_US": "icon_l_en.svg"
            },
            "icon_small": {
                "en_US": "icon_s_en.svg"
            },
            "label": {
                "en_US": "AWS"
            },
            "model_credential_schema": null,
            "models": [
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "J2 Mid V1"
                    },
                    "model": "ai21.j2-mid-v1",
                    "model_properties": {
                        "context_size": 8191,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "topP"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "topP",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 2048,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "maxTokens"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "maxTokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Count Penalty"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "count_penalty",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Presence Penalty"
                            },
                            "max": 5,
                            "min": 0,
                            "name": "presence_penalty",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Frequency Penalty"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "frequency_penalty",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0",
                        "output": "0",
                        "unit": "0.000001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "J2 Ultra V1"
                    },
                    "model": "ai21.j2-ultra-v1",
                    "model_properties": {
                        "context_size": 8191,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "topP"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "topP",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 2048,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "maxTokens"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "maxTokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Count Penalty"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "count_penalty",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Presence Penalty"
                            },
                            "max": 5,
                            "min": 0,
                            "name": "presence_penalty",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Frequency Penalty"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "frequency_penalty",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0",
                        "output": "0",
                        "unit": "0.000001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Jamba 1.5 Large"
                    },
                    "model": "ai21.jamba-1-5-large-v1:0",
                    "model_properties": {
                        "context_size": 256000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 2,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_gen_len"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_gen_len",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.002",
                        "output": "0.008",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Jamba 1.5 Mini"
                    },
                    "model": "ai21.jamba-1-5-mini-v1:0",
                    "model_properties": {
                        "context_size": 256000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 2,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_gen_len"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_gen_len",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.0002",
                        "output": "0.0004",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "tool-call",
                        "stream-tool-call",
                        "vision"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Nova Lite V1"
                    },
                    "model": "amazon.nova-lite-v1:0",
                    "model_properties": {
                        "context_size": 300000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 2048,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_new_tokens"
                            },
                            "max": 5000,
                            "min": 1,
                            "name": "max_new_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Amazon Nova computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Amazon Nova 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00006",
                        "output": "0.00024",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Nova Micro V1"
                    },
                    "model": "amazon.nova-micro-v1:0",
                    "model_properties": {
                        "context_size": 128000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 2048,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_new_tokens"
                            },
                            "max": 5000,
                            "min": 1,
                            "name": "max_new_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Amazon Nova computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Amazon Nova 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.000035",
                        "output": "0.00014",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "tool-call",
                        "stream-tool-call",
                        "vision"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Nova Pro V1"
                    },
                    "model": "amazon.nova-pro-v1:0",
                    "model_properties": {
                        "context_size": 300000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 2048,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_new_tokens"
                            },
                            "max": 5000,
                            "min": 1,
                            "name": "max_new_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Amazon Nova computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Amazon Nova 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.0008",
                        "output": "0.0032",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Titan Text G1 - Express"
                    },
                    "model": "amazon.titan-text-express-v1",
                    "model_properties": {
                        "context_size": 8192,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "topP"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "topP",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 2048,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "maxTokenCount"
                            },
                            "max": 8000,
                            "min": 1,
                            "name": "maxTokenCount",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.0008",
                        "output": "0.0016",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Titan Text G1 - Lite"
                    },
                    "model": "amazon.titan-text-lite-v1",
                    "model_properties": {
                        "context_size": 4096,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "topP"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "topP",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 2048,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "maxTokenCount"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "maxTokenCount",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.0003",
                        "output": "0.0004",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3.5 Haiku"
                    },
                    "model": "anthropic.claude-3-5-haiku-20241022-v1:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 8192,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 8192,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.001",
                        "output": "0.005",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3.5 Sonnet"
                    },
                    "model": "anthropic.claude-3-5-sonnet-20240620-v1:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.003",
                        "output": "0.015",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3.5 Sonnet V2"
                    },
                    "model": "anthropic.claude-3-5-sonnet-20241022-v2:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 8192,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 8192,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.003",
                        "output": "0.015",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3 Haiku"
                    },
                    "model": "anthropic.claude-3-haiku-20240307-v1:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00025",
                        "output": "0.00125",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3 Opus"
                    },
                    "model": "anthropic.claude-3-opus-20240229-v1:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.015",
                        "output": "0.075",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3 Sonnet"
                    },
                    "model": "anthropic.claude-3-sonnet-20240229-v1:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.003",
                        "output": "0.015",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude Instant 1"
                    },
                    "model": "anthropic.claude-instant-v1",
                    "model_properties": {
                        "context_size": 100000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.0008",
                        "output": "0.0024",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": true,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 1"
                    },
                    "model": "anthropic.claude-v1",
                    "model_properties": {
                        "context_size": 100000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.008",
                        "output": "0.024",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 2.1"
                    },
                    "model": "anthropic.claude-v2:1",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.008",
                        "output": "0.024",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 2"
                    },
                    "model": "anthropic.claude-v2",
                    "model_properties": {
                        "context_size": 100000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.008",
                        "output": "0.024",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Command Light Text V14"
                    },
                    "model": "cohere.command-light-text-v14",
                    "model_properties": {
                        "context_size": 4096,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Only sample from the top K options for each subsequent token.",
                                "zh_Hans": "仅从每个后续标记的前 K 个选项中采样。"
                            },
                            "label": {
                                "en_US": "Top k",
                                "zh_Hans": "取样数量"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_tokens"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.0003",
                        "output": "0.0006",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Command R+"
                    },
                    "model": "cohere.command-r-plus-v1:0",
                    "model_properties": {
                        "context_size": 128000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 5,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.75,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "p"
                            },
                            "max": 0.99,
                            "min": 0.01,
                            "name": "p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Only sample from the top K options for each subsequent token.",
                                "zh_Hans": "仅从每个后续标记的前 K 个选项中采样。"
                            },
                            "label": {
                                "en_US": "Top k",
                                "zh_Hans": "取样数量"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Applies a penalty to the log-probability of tokens already in the text.",
                                "ja_JP": "テキストに既に存在するトークンの対数確率にペナルティを適用します。",
                                "pt_BR": "Aplica uma penalidade à probabilidade logarítmica de tokens já presentes no texto.",
                                "zh_Hans": "对文本中已有的标记的对数概率施加惩罚。"
                            },
                            "label": {
                                "en_US": "presence_penalty"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "presence_penalty",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "presence_penalty"
                        },
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Applies a penalty to the log-probability of tokens that appear in the text.",
                                "ja_JP": "テキストに出現するトークンの対数確率にペナルティを適用します。",
                                "pt_BR": "Aplica uma penalidade à probabilidade logarítmica de tokens que aparecem no texto.",
                                "zh_Hans": "对文本中出现的标记的对数概率施加惩罚。"
                            },
                            "label": {
                                "en_US": "frequency_penalty"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "frequency_penalty",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "frequency_penalty"
                        },
                        {
                            "default": 1024,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_tokens"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": false,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "3",
                        "output": "15",
                        "unit": "0.000001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Command R"
                    },
                    "model": "cohere.command-r-v1:0",
                    "model_properties": {
                        "context_size": 128000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 5,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.75,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "p"
                            },
                            "max": 0.99,
                            "min": 0.01,
                            "name": "p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Only sample from the top K options for each subsequent token.",
                                "zh_Hans": "仅从每个后续标记的前 K 个选项中采样。"
                            },
                            "label": {
                                "en_US": "Top k",
                                "zh_Hans": "取样数量"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Applies a penalty to the log-probability of tokens already in the text.",
                                "ja_JP": "テキストに既に存在するトークンの対数確率にペナルティを適用します。",
                                "pt_BR": "Aplica uma penalidade à probabilidade logarítmica de tokens já presentes no texto.",
                                "zh_Hans": "对文本中已有的标记的对数概率施加惩罚。"
                            },
                            "label": {
                                "en_US": "presence_penalty"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "presence_penalty",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "presence_penalty"
                        },
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Applies a penalty to the log-probability of tokens that appear in the text.",
                                "ja_JP": "テキストに出現するトークンの対数確率にペナルティを適用します。",
                                "pt_BR": "Aplica uma penalidade à probabilidade logarítmica de tokens que aparecem no texto.",
                                "zh_Hans": "对文本中出现的标记的对数概率施加惩罚。"
                            },
                            "label": {
                                "en_US": "frequency_penalty"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "frequency_penalty",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "frequency_penalty"
                        },
                        {
                            "default": 1024,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_tokens"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": false,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.5",
                        "output": "1.5",
                        "unit": "0.000001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Command Text V14"
                    },
                    "model": "cohere.command-text-v14",
                    "model_properties": {
                        "context_size": 4096,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Only sample from the top K options for each subsequent token.",
                                "zh_Hans": "仅从每个后续标记的前 K 个选项中采样。"
                            },
                            "label": {
                                "en_US": "Top k",
                                "zh_Hans": "取样数量"
                            },
                            "max": null,
                            "min": null,
                            "name": "k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_tokens"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.0015",
                        "output": "0.002",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3.5 Sonnet(EU.Cross Region Inference)"
                    },
                    "model": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.003",
                        "output": "0.015",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3.5 Sonnet V2(EU.Cross Region Inference)"
                    },
                    "model": "eu.anthropic.claude-3-5-sonnet-20241022-v2:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.003",
                        "output": "0.015",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3 Haiku(EU.Cross Region Inference)"
                    },
                    "model": "eu.anthropic.claude-3-haiku-20240307-v1:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00025",
                        "output": "0.00125",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3 Sonnet(EU.Cross Region Inference)"
                    },
                    "model": "eu.anthropic.claude-3-sonnet-20240229-v1:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.003",
                        "output": "0.015",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Llama 2 Chat 13B"
                    },
                    "model": "meta.llama2-13b-chat-v1",
                    "model_properties": {
                        "context_size": 4096,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 2048,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_gen_len"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "max_gen_len",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00075",
                        "output": "0.001",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Llama 2 Chat 70B"
                    },
                    "model": "meta.llama2-70b-chat-v1",
                    "model_properties": {
                        "context_size": 4096,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 2048,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_gen_len"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "max_gen_len",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00195",
                        "output": "0.00256",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Llama 3.1 405B Instruct"
                    },
                    "model": "meta.llama3-1-405b-instruct-v1:0",
                    "model_properties": {
                        "context_size": 128000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0.5,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.9,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_gen_len"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "max_gen_len",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00532",
                        "output": "0.016",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Llama 3.1 Instruct 70B"
                    },
                    "model": "meta.llama3-1-70b-instruct-v1:0",
                    "model_properties": {
                        "context_size": 128000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0.5,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.9,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_gen_len"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "max_gen_len",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00265",
                        "output": "0.0035",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Llama 3.1 Instruct 8B"
                    },
                    "model": "meta.llama3-1-8b-instruct-v1:0",
                    "model_properties": {
                        "context_size": 128000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0.5,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.9,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_gen_len"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "max_gen_len",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.0003",
                        "output": "0.0006",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Llama 3 Instruct 70B"
                    },
                    "model": "meta.llama3-70b-instruct-v1:0",
                    "model_properties": {
                        "context_size": 8192,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_gen_len"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "max_gen_len",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00265",
                        "output": "0.0035",
                        "unit": "0.00001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Llama 3 Instruct 8B"
                    },
                    "model": "meta.llama3-8b-instruct-v1:0",
                    "model_properties": {
                        "context_size": 8192,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_gen_len"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "max_gen_len",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.0004",
                        "output": "0.0006",
                        "unit": "0.0001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Mistral 7B Instruct"
                    },
                    "model": "mistral.mistral-7b-instruct-v0:2",
                    "model_properties": {
                        "context_size": 32000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0.5,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.9,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 50,
                            "help": {
                                "en_US": "Only sample from the top K options for each subsequent token.",
                                "zh_Hans": "仅从每个后续标记的前 K 个选项中采样。"
                            },
                            "label": {
                                "en_US": "Top k",
                                "zh_Hans": "取样数量"
                            },
                            "max": 200,
                            "min": 1,
                            "name": "top_k",
                            "options": [],
                            "precision": 0,
                            "required": false,
                            "type": "int",
                            "use_template": "top_k"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_tokens"
                            },
                            "max": 8192,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00015",
                        "output": "0.0002",
                        "unit": "0.00001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "tool-call",
                        "agent-thought"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Mistral Large"
                    },
                    "model": "mistral.mistral-large-2402-v1:0",
                    "model_properties": {
                        "context_size": 32000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0.7,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_tokens"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.008",
                        "output": "0.024",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Mistral Large 2 (24.07)"
                    },
                    "model": "mistral.mistral-large-2407-v1:0",
                    "model_properties": {
                        "context_size": 128000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0.7,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_tokens"
                            },
                            "max": 8192,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.003",
                        "output": "0.009",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Mistral Small"
                    },
                    "model": "mistral.mistral-small-2402-v1:0",
                    "model_properties": {
                        "context_size": 32000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0.7,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_tokens"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.001",
                        "output": "0.03",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Mixtral 8X7B Instruct"
                    },
                    "model": "mistral.mixtral-8x7b-instruct-v0:1",
                    "model_properties": {
                        "context_size": 32000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0.5,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.9,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 50,
                            "help": {
                                "en_US": "Only sample from the top K options for each subsequent token.",
                                "zh_Hans": "仅从每个后续标记的前 K 个选项中采样。"
                            },
                            "label": {
                                "en_US": "Top k",
                                "zh_Hans": "取样数量"
                            },
                            "max": 200,
                            "min": 1,
                            "name": "top_k",
                            "options": [],
                            "precision": 0,
                            "required": false,
                            "type": "int",
                            "use_template": "top_k"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_tokens"
                            },
                            "max": 8192,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00045",
                        "output": "0.0007",
                        "unit": "0.00001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "tool-call",
                        "stream-tool-call",
                        "vision"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Nova Lite V1 (US.Cross Region Inference)"
                    },
                    "model": "us.amazon.nova-lite-v1:0",
                    "model_properties": {
                        "context_size": 300000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 2048,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_new_tokens"
                            },
                            "max": 5000,
                            "min": 1,
                            "name": "max_new_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Amazon Nova computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Amazon Nova 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00006",
                        "output": "0.00024",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Nova Micro V1 (US.Cross Region Inference)"
                    },
                    "model": "us.amazon.nova-micro-v1:0",
                    "model_properties": {
                        "context_size": 128000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 2048,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_new_tokens"
                            },
                            "max": 5000,
                            "min": 1,
                            "name": "max_new_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Amazon Nova computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Amazon Nova 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.000035",
                        "output": "0.00014",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "tool-call",
                        "stream-tool-call",
                        "vision"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Nova Pro V1 (US.Cross Region Inference)"
                    },
                    "model": "us.amazon.nova-pro-v1:0",
                    "model_properties": {
                        "context_size": 300000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 2048,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_new_tokens"
                            },
                            "max": 5000,
                            "min": 1,
                            "name": "max_new_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Amazon Nova computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Amazon Nova 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.0008",
                        "output": "0.0032",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3.5 Haiku(US.Cross Region Inference)"
                    },
                    "model": "us.anthropic.claude-3-5-haiku-20241022-v1:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 8192,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 8192,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.001",
                        "output": "0.005",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3.5 Sonnet(US.Cross Region Inference)"
                    },
                    "model": "us.anthropic.claude-3-5-sonnet-20240620-v1:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.003",
                        "output": "0.015",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3.5 Sonnet V2(US.Cross Region Inference)"
                    },
                    "model": "us.anthropic.claude-3-5-sonnet-20241022-v2:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 8192,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 8192,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.003",
                        "output": "0.015",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3.7 Sonnet(US.Cross Region Inference)"
                    },
                    "model": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": false,
                            "help": {
                                "en_US": "Controls the model's reasoning capability. When enabled, temperature will be fixed to 1 and top_p will be disabled.",
                                "zh_Hans": "控制模型的推理能力。启用时，temperature将固定为1且top_p将被禁用。"
                            },
                            "label": {
                                "en_US": "Reasoning Type",
                                "zh_Hans": "推理配置"
                            },
                            "max": null,
                            "min": null,
                            "name": "reasoning_type",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "boolean",
                            "use_template": null
                        },
                        {
                            "default": 1024,
                            "help": {
                                "en_US": "Budget limit for reasoning (minimum 1024), must be less than max_tokens. Only available when reasoning type is enabled.",
                                "zh_Hans": "推理的预算限制（最小1024），必须小于max_tokens。仅在推理类型为enabled时可用。"
                            },
                            "label": {
                                "en_US": "Reasoning Budget",
                                "zh_Hans": "推理预算"
                            },
                            "max": 128000,
                            "min": 0,
                            "name": "reasoning_budget",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": 8192,
                            "help": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "label": {
                                "en_US": "Max Tokens",
                                "zh_Hans": "最大token数"
                            },
                            "max": 128000,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "The amount of randomness injected into the response. When reasoning is enabled, this value will be fixed to 1.",
                                "zh_Hans": "生成内容的随机性。当推理功能启用时，该值将被固定为1。"
                            },
                            "label": {
                                "en_US": "Model Temperature",
                                "zh_Hans": "模型温度"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": {
                                "en_US": "The probability threshold in nucleus sampling. When reasoning is enabled, this parameter will be disabled.",
                                "zh_Hans": "在核采样中的概率阈值。当推理功能启用时，该参数将被禁用。"
                            },
                            "label": {
                                "en_US": "Top P",
                                "zh_Hans": "Top P"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 0,
                            "help": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "label": {
                                "en_US": "Top k",
                                "zh_Hans": "取样数量"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.003",
                        "output": "0.015",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3 Haiku(US.Cross Region Inference)"
                    },
                    "model": "us.anthropic.claude-3-haiku-20240307-v1:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00025",
                        "output": "0.00125",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3 Opus(US.Cross Region Inference)"
                    },
                    "model": "us.anthropic.claude-3-opus-20240229-v1:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.015",
                        "output": "0.075",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "Claude 3 Sonnet(US.Cross Region Inference)"
                    },
                    "model": "us.anthropic.claude-3-sonnet-20240229-v1:0",
                    "model_properties": {
                        "context_size": 200000,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 4096,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "The maximum number of tokens to generate before stopping. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens. Different Anthropic Claude models have different maximum values for this parameter.",
                                "zh_Hans": "停止前生成的最大令牌数。请注意，Anthropic Claude 模型可能会在达到 max_tokens 的值之前停止生成令牌。不同的 Anthropic Claude 模型对此参数具有不同的最大值。"
                            },
                            "max": 4096,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "The amount of randomness injected into the response.",
                                "zh_Hans": "生成内容的随机性。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": null,
                            "label": {
                                "en_US": "In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.",
                                "zh_Hans": "在核采样中，Anthropic Claude 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "float",
                            "use_template": null
                        },
                        {
                            "default": 0,
                            "help": null,
                            "label": {
                                "en_US": "Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.",
                                "zh_Hans": "对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。"
                            },
                            "max": 500,
                            "min": 0,
                            "name": "top_k",
                            "options": [],
                            "precision": null,
                            "required": false,
                            "type": "int",
                            "use_template": null
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.003",
                        "output": "0.015",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "agent-thought",
                        "vision",
                        "tool-call",
                        "stream-tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "DeepSeek-R1(US.Cross Region Inference)"
                    },
                    "model": "us.deepseek.r1-v1:0",
                    "model_properties": {
                        "context_size": 32768,
                        "mode": "chat"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 8192,
                            "help": {
                                "en_US": "The maximum number of tokens to generate before stopping.",
                                "zh_Hans": "停止前生成的最大令牌数。"
                            },
                            "label": {
                                "en_US": "Max Tokens",
                                "zh_Hans": "最大token数"
                            },
                            "max": 128000,
                            "min": 1,
                            "name": "max_tokens",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "The amount of randomness injected into the response. When reasoning is enabled, this value will be fixed to 1.",
                                "zh_Hans": "生成内容的随机性。当推理功能启用时，该值将被固定为1。"
                            },
                            "label": {
                                "en_US": "Model Temperature",
                                "zh_Hans": "模型温度"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.999,
                            "help": {
                                "en_US": "The probability threshold in nucleus sampling. When reasoning is enabled, this parameter will be disabled.",
                                "zh_Hans": "在核采样中的概率阈值。当推理功能启用时，该参数将被禁用。"
                            },
                            "label": {
                                "en_US": "Top P",
                                "zh_Hans": "Top P"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": null,
                            "help": {
                                "en_US": "Set a response format, ensure the output from llm is a valid code block as possible, such as JSON, XML, etc.",
                                "ja_JP": "応答形式を設定します。llmの出力が可能な限り有効なコードブロックであることを確認します。",
                                "pt_BR": "Defina um formato de resposta para garantir que a saída do llm seja um bloco de código válido o mais possível, como JSON, XML, etc.",
                                "zh_Hans": "设置一个返回格式，确保llm的输出尽可能是有效的代码块，如JSON、XML等"
                            },
                            "label": {
                                "en_US": "response_format"
                            },
                            "max": null,
                            "min": null,
                            "name": "response_format",
                            "options": [
                                "JSON",
                                "XML"
                            ],
                            "precision": null,
                            "required": false,
                            "type": "string",
                            "use_template": "response_format"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.001",
                        "output": "0.005",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "US Meta Llama 3.1 Instruct 70B"
                    },
                    "model": "us.meta.llama3-1-70b-instruct-v1:0",
                    "model_properties": {
                        "context_size": 128000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0.5,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.9,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_gen_len"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "max_gen_len",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00265",
                        "output": "0.0035",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "vision",
                        "tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "US Meta Llama 3.2 11B Instruct"
                    },
                    "model": "us.meta.llama3-2-11b-instruct-v1:0",
                    "model_properties": {
                        "context_size": 128000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0.5,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_gen_len"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "max_gen_len",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00035",
                        "output": "0.00035",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "US Meta Llama 3.2 1B Instruct"
                    },
                    "model": "us.meta.llama3-2-1b-instruct-v1:0",
                    "model_properties": {
                        "context_size": 128000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0.5,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_gen_len"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "max_gen_len",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.0001",
                        "output": "0.0001",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "US Meta Llama 3.2 3B Instruct"
                    },
                    "model": "us.meta.llama3-2-3b-instruct-v1:0",
                    "model_properties": {
                        "context_size": 128000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0.5,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 1,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_gen_len"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "max_gen_len",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00015",
                        "output": "0.00015",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": [
                        "tool-call"
                    ],
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "US Meta Llama 3.2 90B Instruct"
                    },
                    "model": "us.meta.llama3-2-90b-instruct-v1:0",
                    "model_properties": {
                        "context_size": 128000,
                        "mode": "completion"
                    },
                    "model_type": "llm",
                    "parameter_rules": [
                        {
                            "default": 0.5,
                            "help": {
                                "en_US": "Controls randomness. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions.",
                                "ja_JP": "温度はランダム性を制御します。温度が低いほどランダムな完成が少なくなります。温度がゼロに近づくと、モデルは決定論的で繰り返しになります。温度が高いほどランダムな完成が多くなります。",
                                "pt_BR": "A temperatura controla a aleatoriedade. Menores temperaturas resultam em menos conclusões aleatórias. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo. Temperaturas mais altas resultam em mais conclusões aleatórias.",
                                "zh_Hans": "温度控制随机性。较低的温度会导致较少的随机完成。随着温度接近零，模型将变得确定性和重复性。较高的温度会导致更多的随机完成。"
                            },
                            "label": {
                                "en_US": "temperature"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "temperature",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "temperature"
                        },
                        {
                            "default": 0.9,
                            "help": {
                                "en_US": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
                                "ja_JP": "核サンプリングを通じて多様性を制御します：0.5は、すべての可能性加权オプションの半分を考慮します。",
                                "pt_BR": "Controla a diversidade via amostragem de núcleo: 0.5 significa que metade das opções com maior probabilidade são consideradas.",
                                "zh_Hans": "通过核心采样控制多样性：0.5表示考虑了一半的所有可能性加权选项。"
                            },
                            "label": {
                                "en_US": "top_p"
                            },
                            "max": 1,
                            "min": 0,
                            "name": "top_p",
                            "options": [],
                            "precision": 2,
                            "required": false,
                            "type": "float",
                            "use_template": "top_p"
                        },
                        {
                            "default": 512,
                            "help": {
                                "en_US": "Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.",
                                "ja_JP": "生成結果の長さの上限を指定します。生成結果が切り捨てられた場合は、このパラメータを大きくすることができます。",
                                "pt_BR": "Especifica o limite superior para o comprimento dos resultados gerados. Se os resultados gerados forem truncados, você pode aumentar este parâmetro.",
                                "zh_Hans": "指定生成结果长度的上限。如果生成结果截断，可以调大该参数。"
                            },
                            "label": {
                                "en_US": "max_gen_len"
                            },
                            "max": 2048,
                            "min": 1,
                            "name": "max_gen_len",
                            "options": [],
                            "precision": 0,
                            "required": true,
                            "type": "int",
                            "use_template": "max_tokens"
                        }
                    ],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.002",
                        "output": "0.002",
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "amazon.rerank-v1:0"
                    },
                    "model": "amazon.rerank-v1:0",
                    "model_properties": {
                        "context_size": 5120
                    },
                    "model_type": "rerank",
                    "parameter_rules": [],
                    "pricing": null
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "cohere.rerank-v3-5:0"
                    },
                    "model": "cohere.rerank-v3-5:0",
                    "model_properties": {
                        "context_size": 5120
                    },
                    "model_type": "rerank",
                    "parameter_rules": [],
                    "pricing": null
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "amazon.titan-embed-text-v1"
                    },
                    "model": "amazon.titan-embed-text-v1",
                    "model_properties": {
                        "context_size": 8192
                    },
                    "model_type": "text-embedding",
                    "parameter_rules": [],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.0001",
                        "output": null,
                        "unit": "0.0001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "amazon.titan-embed-text-v2:0"
                    },
                    "model": "amazon.titan-embed-text-v2:0",
                    "model_properties": {
                        "context_size": 8192
                    },
                    "model_type": "text-embedding",
                    "parameter_rules": [],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.00002",
                        "output": null,
                        "unit": "0.00001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "cohere.embed-english-v3"
                    },
                    "model": "cohere.embed-english-v3",
                    "model_properties": {
                        "context_size": 512
                    },
                    "model_type": "text-embedding",
                    "parameter_rules": [],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.1",
                        "output": null,
                        "unit": "0.000001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "cohere.embed-multilingual-v3"
                    },
                    "model": "cohere.embed-multilingual-v3",
                    "model_properties": {
                        "context_size": 512
                    },
                    "model_type": "text-embedding",
                    "parameter_rules": [],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.1",
                        "output": null,
                        "unit": "0.000001"
                    }
                }
            ],
            "position": {},
            "provider": "bedrock",
            "provider_credential_schema": {
                "credential_form_schemas": [
                    {
                        "default": null,
                        "label": {
                            "en_US": "Access Key (If not provided, credentials are obtained from the running environment.)",
                            "zh_Hans": "Access Key"
                        },
                        "max_length": 0,
                        "options": [],
                        "placeholder": {
                            "en_US": "Enter your Access Key",
                            "zh_Hans": "在此输入您的 Access Key"
                        },
                        "required": false,
                        "show_on": [],
                        "type": "secret-input",
                        "variable": "aws_access_key_id"
                    },
                    {
                        "default": null,
                        "label": {
                            "en_US": "Secret Access Key",
                            "zh_Hans": "Secret Access Key"
                        },
                        "max_length": 0,
                        "options": [],
                        "placeholder": {
                            "en_US": "Enter your Secret Access Key",
                            "zh_Hans": "在此输入您的 Secret Access Key"
                        },
                        "required": false,
                        "show_on": [],
                        "type": "secret-input",
                        "variable": "aws_secret_access_key"
                    },
                    {
                        "default": "us-east-1",
                        "label": {
                            "en_US": "AWS Region",
                            "ja_JP": "AWS リージョン",
                            "zh_Hans": "AWS 地区"
                        },
                        "max_length": 0,
                        "options": [
                            {
                                "label": {
                                    "en_US": "US East (N. Virginia)",
                                    "ja_JP": "米国 (バージニア北部)",
                                    "zh_Hans": "美国东部 (弗吉尼亚北部)"
                                },
                                "show_on": [],
                                "value": "us-east-1"
                            },
                            {
                                "label": {
                                    "en_US": "US East (Ohio)",
                                    "ja_JP": "米国 (オハイオ)",
                                    "zh_Hans": "美国东部 (俄亥俄)"
                                },
                                "show_on": [],
                                "value": "us-east-2"
                            },
                            {
                                "label": {
                                    "en_US": "US West (Oregon)",
                                    "ja_JP": "米国 (オレゴン)",
                                    "zh_Hans": "美国西部 (俄勒冈州)"
                                },
                                "show_on": [],
                                "value": "us-west-2"
                            },
                            {
                                "label": {
                                    "en_US": "Asia Pacific (Mumbai)",
                                    "ja_JP": "アジアパシフィック (ムンバイ)",
                                    "zh_Hans": "亚太地区（孟买）"
                                },
                                "show_on": [],
                                "value": "ap-south-1"
                            },
                            {
                                "label": {
                                    "en_US": "Asia Pacific (Singapore)",
                                    "ja_JP": "アジアパシフィック (シンガポール)",
                                    "zh_Hans": "亚太地区 (新加坡)"
                                },
                                "show_on": [],
                                "value": "ap-southeast-1"
                            },
                            {
                                "label": {
                                    "en_US": "Asia Pacific (Sydney)",
                                    "ja_JP": "アジアパシフィック (シドニー)",
                                    "zh_Hans": "亚太地区 (悉尼)"
                                },
                                "show_on": [],
                                "value": "ap-southeast-2"
                            },
                            {
                                "label": {
                                    "en_US": "Asia Pacific (Tokyo)",
                                    "ja_JP": "アジアパシフィック (東京)",
                                    "zh_Hans": "亚太地区 (东京)"
                                },
                                "show_on": [],
                                "value": "ap-northeast-1"
                            },
                            {
                                "label": {
                                    "en_US": "Asia Pacific (Seoul)",
                                    "ja_JP": "アジアパシフィック (ソウル)",
                                    "zh_Hans": "亚太地区（首尔）"
                                },
                                "show_on": [],
                                "value": "ap-northeast-2"
                            },
                            {
                                "label": {
                                    "en_US": "Canada (Central)",
                                    "ja_JP": "カナダ (中部)",
                                    "zh_Hans": "加拿大（中部）"
                                },
                                "show_on": [],
                                "value": "ca-central-1"
                            },
                            {
                                "label": {
                                    "en_US": "Europe (Frankfurt)",
                                    "ja_JP": "欧州 (フランクフルト)",
                                    "zh_Hans": "欧洲 (法兰克福)"
                                },
                                "show_on": [],
                                "value": "eu-central-1"
                            },
                            {
                                "label": {
                                    "en_US": "Europe (Ireland)",
                                    "ja_JP": "欧州 (アイルランド)",
                                    "zh_Hans": "欧洲（爱尔兰）"
                                },
                                "show_on": [],
                                "value": "eu-west-1"
                            },
                            {
                                "label": {
                                    "en_US": "Europe (London)",
                                    "ja_JP": "欧州 (ロンドン)",
                                    "zh_Hans": "欧洲西部 (伦敦)"
                                },
                                "show_on": [],
                                "value": "eu-west-2"
                            },
                            {
                                "label": {
                                    "en_US": "Europe (Paris)",
                                    "ja_JP": "欧州 (パリ)",
                                    "zh_Hans": "欧洲（巴黎）"
                                },
                                "show_on": [],
                                "value": "eu-west-3"
                            },
                            {
                                "label": {
                                    "en_US": "South America (São Paulo)",
                                    "ja_JP": "南米 (サンパウロ)",
                                    "zh_Hans": "南美洲（圣保罗）"
                                },
                                "show_on": [],
                                "value": "sa-east-1"
                            },
                            {
                                "label": {
                                    "en_US": "AWS GovCloud (US-West)",
                                    "ja_JP": "AWS GovCloud (米国西部)",
                                    "zh_Hans": "AWS GovCloud (US-West)"
                                },
                                "show_on": [],
                                "value": "us-gov-west-1"
                            }
                        ],
                        "placeholder": null,
                        "required": true,
                        "show_on": [],
                        "type": "select",
                        "variable": "aws_region"
                    },
                    {
                        "default": null,
                        "label": {
                            "en_US": "Bedrock Endpoint URL",
                            "zh_Hans": "Bedrock Endpoint URL"
                        },
                        "max_length": 0,
                        "options": [],
                        "placeholder": {
                            "en_US": "Enter your Bedrock Endpoint URL, e.g. https://123456.cloudfront.net",
                            "zh_Hans": "在此输入您的 Bedrock Endpoint URL, 如：https://123456.cloudfront.net"
                        },
                        "required": false,
                        "show_on": [],
                        "type": "text-input",
                        "variable": "bedrock_endpoint_url"
                    },
                    {
                        "default": null,
                        "label": {
                            "en_US": "Available Model Name",
                            "zh_Hans": "可用模型名称"
                        },
                        "max_length": 0,
                        "options": [],
                        "placeholder": {
                            "en_US": "A model you have access to (e.g. amazon.titan-text-lite-v1) for validation.",
                            "zh_Hans": "为了进行验证，请输入一个您可用的模型名称 (例如：amazon.titan-text-lite-v1)"
                        },
                        "required": false,
                        "show_on": [],
                        "type": "text-input",
                        "variable": "model_for_validation"
                    }
                ]
            },
            "supported_model_types": [
                "llm",
                "text-embedding",
                "rerank"
            ]
        },
        "tool": {},
        "agent_strategy": {},
        "install_count": 1994,
        "version_updated_at": "2025-03-14T14:52:44.902797Z",
        "latest_version": "0.0.9",
        "latest_package_identifier": "langgenius/bedrock:0.0.9@4484b4097187376df28408110fa951c64a18588689746ed4eefc4ee4dc3ec9f2",
        "tags": []
    },
    {
        "type": "plugin",
        "name": "azure_openai",
        "org": "langgenius",
        "plugin_id": "langgenius/azure_openai",
        "icon": "langgenius/packages/azure_openai/_assets/icon_s_en.svg",
        "label": {
            "en_US": "Azure OpenAI"
        },
        "brief": {
            "en_US": "Azure OpenAI Service Model"
        },
        "introduction": "## Overview\nAzure OpenAI Service is a cloud-based platform that provides access to advanced AI models developed by OpenAI, integrated with Microsoft's Azure infrastructure. This plugin allows users to leverage cutting-edge generative AI capabilities such as LLMs, text embedding, speech-to-text (STT), and text-to-speech (TTS) for various applications, ensuring security and compliance through Azure's robust framework.\n\n## Configure\nOnce the plugin is installed, configure your Azure OpenAI Service Model by providing the Model Type, Deployment Name, API Endpoint URL, your API Key, the API Version, and the Base Model. Find your API key on Azure. Save to complete the setup.\n\n<img src=\"./_assets/azure_openai-01.png\" width=\"400\" />\n",
        "category": "model",
        "repository": "",
        "resource": {
            "memory": 268435456,
            "permission": {
                "model": {
                    "enabled": false,
                    "llm": false,
                    "moderation": false,
                    "rerank": false,
                    "speech2text": false,
                    "text_embedding": false,
                    "tts": false
                }
            }
        },
        "privacy_options": "",
        "privacy_policy": "",
        "created_at": "2024-12-13T13:52:33.571197Z",
        "updated_at": "2025-03-11T01:21:34.699285Z",
        "badges": [],
        "plugins": {
            "agent_strategies": null,
            "endpoints": null,
            "models": [
                "provider/azure_openai.yaml"
            ],
            "tools": null
        },
        "endpoint": {},
        "model": {
            "background": "#E3F0FF",
            "configurate_methods": [
                "customizable-model"
            ],
            "description": {
                "en_US": "Azure OpenAI Service Model"
            },
            "help": {
                "title": {
                    "en_US": "Get your API key from Azure",
                    "zh_Hans": "从 Azure 获取 API Key"
                },
                "url": {
                    "en_US": "https://azure.microsoft.com/en-us/products/ai-services/openai-service"
                }
            },
            "icon_large": {
                "en_US": "icon_l_en.png"
            },
            "icon_small": {
                "en_US": "icon_s_en.svg"
            },
            "label": {
                "en_US": "Azure OpenAI Service Model"
            },
            "model_credential_schema": {
                "credential_form_schemas": [
                    {
                        "default": null,
                        "label": {
                            "en_US": "API Endpoint URL",
                            "zh_Hans": "API 域名"
                        },
                        "max_length": 0,
                        "options": [],
                        "placeholder": {
                            "en_US": "Enter your API Endpoint, eg: https://example.com/xxx",
                            "zh_Hans": "在此输入您的 API 域名，如：https://example.com/xxx"
                        },
                        "required": true,
                        "show_on": [],
                        "type": "text-input",
                        "variable": "openai_api_base"
                    },
                    {
                        "default": null,
                        "label": {
                            "en_US": "API Key",
                            "zh_Hans": "API Key"
                        },
                        "max_length": 0,
                        "options": [],
                        "placeholder": {
                            "en_US": "Enter your API key here",
                            "zh_Hans": "在此输入您的 API Key"
                        },
                        "required": true,
                        "show_on": [],
                        "type": "secret-input",
                        "variable": "openai_api_key"
                    },
                    {
                        "default": null,
                        "label": {
                            "en_US": "API Version",
                            "zh_Hans": "API 版本"
                        },
                        "max_length": 0,
                        "options": [
                            {
                                "label": {
                                    "en_US": "2025-02-01-preview"
                                },
                                "show_on": [],
                                "value": "2025-02-01-preview"
                            },
                            {
                                "label": {
                                    "en_US": "2025-01-01-preview"
                                },
                                "show_on": [],
                                "value": "2025-01-01-preview"
                            },
                            {
                                "label": {
                                    "en_US": "2024-12-01-preview"
                                },
                                "show_on": [],
                                "value": "2024-12-01-preview"
                            },
                            {
                                "label": {
                                    "en_US": "2024-10-01-preview"
                                },
                                "show_on": [],
                                "value": "2024-10-01-preview"
                            },
                            {
                                "label": {
                                    "en_US": "2024-09-01-preview"
                                },
                                "show_on": [],
                                "value": "2024-09-01-preview"
                            },
                            {
                                "label": {
                                    "en_US": "2024-08-01-preview"
                                },
                                "show_on": [],
                                "value": "2024-08-01-preview"
                            },
                            {
                                "label": {
                                    "en_US": "2024-07-01-preview"
                                },
                                "show_on": [],
                                "value": "2024-07-01-preview"
                            },
                            {
                                "label": {
                                    "en_US": "2024-05-01-preview"
                                },
                                "show_on": [],
                                "value": "2024-05-01-preview"
                            },
                            {
                                "label": {
                                    "en_US": "2024-04-01-preview"
                                },
                                "show_on": [],
                                "value": "2024-04-01-preview"
                            },
                            {
                                "label": {
                                    "en_US": "2024-03-01-preview"
                                },
                                "show_on": [],
                                "value": "2024-03-01-preview"
                            },
                            {
                                "label": {
                                    "en_US": "2024-02-15-preview"
                                },
                                "show_on": [],
                                "value": "2024-02-15-preview"
                            },
                            {
                                "label": {
                                    "en_US": "2023-12-01-preview"
                                },
                                "show_on": [],
                                "value": "2023-12-01-preview"
                            },
                            {
                                "label": {
                                    "en_US": "2024-02-01"
                                },
                                "show_on": [],
                                "value": "2024-02-01"
                            },
                            {
                                "label": {
                                    "en_US": "2024-06-01"
                                },
                                "show_on": [],
                                "value": "2024-06-01"
                            }
                        ],
                        "placeholder": {
                            "en_US": "Select your API Version here",
                            "zh_Hans": "在此选择您的 API 版本"
                        },
                        "required": true,
                        "show_on": [],
                        "type": "select",
                        "variable": "openai_api_version"
                    },
                    {
                        "default": null,
                        "label": {
                            "en_US": "Base Model",
                            "zh_Hans": "基础模型"
                        },
                        "max_length": 0,
                        "options": [
                            {
                                "label": {
                                    "en_US": "gpt-35-turbo"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-35-turbo"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-35-turbo-0125"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-35-turbo-0125"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-35-turbo-16k"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-35-turbo-16k"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-4"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-4"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-4-32k"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-4-32k"
                            },
                            {
                                "label": {
                                    "en_US": "o3-mini"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "o3-mini"
                            },
                            {
                                "label": {
                                    "en_US": "o1-mini"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "o1-mini"
                            },
                            {
                                "label": {
                                    "en_US": "o1-preview"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "o1-preview"
                            },
                            {
                                "label": {
                                    "en_US": "o1"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "o1"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-4o-mini"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-4o-mini"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-4o-mini-2024-07-18"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-4o-mini-2024-07-18"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-4o"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-4o"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-4o-2024-05-13"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-4o-2024-05-13"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-4o-2024-08-06"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-4o-2024-08-06"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-4-turbo"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-4-turbo"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-4-turbo-2024-04-09"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-4-turbo-2024-04-09"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-4o-2024-11-20"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-4o-2024-11-20"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-4-0125-preview"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-4-0125-preview"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-4-1106-preview"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-4-1106-preview"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-4-vision-preview"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-4-vision-preview"
                            },
                            {
                                "label": {
                                    "en_US": "gpt-35-turbo-instruct"
                                },
                                "show_on": [
                                    {
                                        "value": "llm",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "gpt-35-turbo-instruct"
                            },
                            {
                                "label": {
                                    "en_US": "text-embedding-ada-002"
                                },
                                "show_on": [
                                    {
                                        "value": "text-embedding",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "text-embedding-ada-002"
                            },
                            {
                                "label": {
                                    "en_US": "text-embedding-3-small"
                                },
                                "show_on": [
                                    {
                                        "value": "text-embedding",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "text-embedding-3-small"
                            },
                            {
                                "label": {
                                    "en_US": "text-embedding-3-large"
                                },
                                "show_on": [
                                    {
                                        "value": "text-embedding",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "text-embedding-3-large"
                            },
                            {
                                "label": {
                                    "en_US": "whisper-1"
                                },
                                "show_on": [
                                    {
                                        "value": "speech2text",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "whisper-1"
                            },
                            {
                                "label": {
                                    "en_US": "tts-1"
                                },
                                "show_on": [
                                    {
                                        "value": "tts",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "tts-1"
                            },
                            {
                                "label": {
                                    "en_US": "tts-1-hd"
                                },
                                "show_on": [
                                    {
                                        "value": "tts",
                                        "variable": "__model_type"
                                    }
                                ],
                                "value": "tts-1-hd"
                            }
                        ],
                        "placeholder": {
                            "en_US": "Enter your model version",
                            "zh_Hans": "在此输入您的模型版本"
                        },
                        "required": true,
                        "show_on": [],
                        "type": "select",
                        "variable": "base_model_name"
                    }
                ],
                "model": {
                    "label": {
                        "en_US": "Deployment Name",
                        "zh_Hans": "部署名称"
                    },
                    "placeholder": {
                        "en_US": "Enter your Deployment Name here, matching the Azure deployment name.",
                        "zh_Hans": "在此输入您的部署名称，与 Azure 部署名称匹配。"
                    }
                }
            },
            "models": [],
            "position": {},
            "provider": "azure_openai",
            "provider_credential_schema": null,
            "supported_model_types": [
                "llm",
                "text-embedding",
                "speech2text",
                "tts"
            ]
        },
        "tool": {},
        "agent_strategy": {},
        "install_count": 5217,
        "version_updated_at": "2025-03-11T01:21:34.699248Z",
        "latest_version": "0.0.8",
        "latest_package_identifier": "langgenius/azure_openai:0.0.8@59c461863a2753aaa07c3fad5831b2026232920342cced3781f94ae92d220eb4",
        "tags": []
    },
    {
        "type": "plugin",
        "name": "jina",
        "org": "langgenius",
        "plugin_id": "langgenius/jina",
        "icon": "langgenius/packages/jina/_assets/icon_s_en.svg",
        "label": {
            "en_US": "Jina"
        },
        "brief": {
            "en_US": "Embedding and Rerank Model Supported"
        },
        "introduction": "## Overview\n\nJina is a cloud-native neural search framework for building scalable, multimodal AI applications. It offers a robust ecosystem for indexing, querying, and retrieving various data types. Jina utilizes modular \"Pods\" for flexible search pipelines and supports diverse backends. Key components include `jina-clip-v2`, a model offering multilingual multimodal embeddings for text and images, and `jina-embeddings-v3`, providing multilingual embeddings with Task LoRA for fine-tuning. Jina enables developers to create sophisticated search, recommendation, and QA systems, streamlining AI development by making advanced functionalities more accessible and scalable.\n\n## Configure\n\nAfter installing the plugin, configure the API key and API base within the Model Provider settings. Obtain your API key from [here](https://jina.ai/). Once saved, you can begin using Jina to build your AI agents and agentic workflows.\n\n![](./_assets/jina_config.PNG)",
        "category": "model",
        "repository": "",
        "resource": {
            "memory": 1048576,
            "permission": {
                "model": {
                    "enabled": true,
                    "llm": true,
                    "moderation": false,
                    "rerank": false,
                    "speech2text": false,
                    "text_embedding": false,
                    "tts": false
                },
                "tool": {
                    "enabled": true
                }
            }
        },
        "privacy_options": "",
        "privacy_policy": "",
        "created_at": "2024-11-29T11:26:04.136267Z",
        "updated_at": "2025-03-13T02:54:34.726091Z",
        "badges": [],
        "plugins": {
            "agent_strategies": null,
            "endpoints": null,
            "models": [
                "provider/jina.yaml"
            ],
            "tools": null
        },
        "endpoint": {},
        "model": {
            "background": "#EFFDFD",
            "configurate_methods": [
                "predefined-model",
                "customizable-model"
            ],
            "description": {
                "en_US": "Embedding and Rerank Model Supported"
            },
            "help": {
                "title": {
                    "en_US": "Get your API key from Jina AI",
                    "zh_Hans": "从 Jina 获取 API Key"
                },
                "url": {
                    "en_US": "https://jina.ai/"
                }
            },
            "icon_large": {
                "en_US": "icon_l_en.svg"
            },
            "icon_small": {
                "en_US": "icon_s_en.svg"
            },
            "label": {
                "en_US": "Jina"
            },
            "model_credential_schema": {
                "credential_form_schemas": [
                    {
                        "default": null,
                        "label": {
                            "en_US": "API Key"
                        },
                        "max_length": 0,
                        "options": [],
                        "placeholder": {
                            "en_US": "Enter your API Key",
                            "zh_Hans": "在此输入您的 API Key"
                        },
                        "required": true,
                        "show_on": [],
                        "type": "secret-input",
                        "variable": "api_key"
                    },
                    {
                        "default": null,
                        "label": {
                            "en_US": "Base URL",
                            "zh_Hans": "服务器 URL"
                        },
                        "max_length": 0,
                        "options": [],
                        "placeholder": {
                            "en_US": "Base URL, e.g. https://api.jina.ai/v1",
                            "zh_Hans": "Base URL, e.g. https://api.jina.ai/v1"
                        },
                        "required": false,
                        "show_on": [],
                        "type": "text-input",
                        "variable": "base_url"
                    },
                    {
                        "default": "8192",
                        "label": {
                            "en_US": "Context size",
                            "zh_Hans": "上下文大小"
                        },
                        "max_length": 0,
                        "options": [],
                        "placeholder": {
                            "en_US": "Enter context size",
                            "zh_Hans": "输入上下文大小"
                        },
                        "required": false,
                        "show_on": [],
                        "type": "text-input",
                        "variable": "context_size"
                    }
                ],
                "model": {
                    "label": {
                        "en_US": "Model Name",
                        "zh_Hans": "模型名称"
                    },
                    "placeholder": {
                        "en_US": "Enter your model name",
                        "zh_Hans": "输入模型名称"
                    }
                }
            },
            "models": [
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "jina-colbert-v1-en"
                    },
                    "model": "jina-colbert-v1-en",
                    "model_properties": {
                        "context_size": 8192
                    },
                    "model_type": "rerank",
                    "parameter_rules": [],
                    "pricing": null
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "jina-reranker-v1-base-en"
                    },
                    "model": "jina-reranker-v1-base-en",
                    "model_properties": {
                        "context_size": 8192
                    },
                    "model_type": "rerank",
                    "parameter_rules": [],
                    "pricing": null
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "jina-reranker-v1-tiny-en"
                    },
                    "model": "jina-reranker-v1-tiny-en",
                    "model_properties": {
                        "context_size": 8192
                    },
                    "model_type": "rerank",
                    "parameter_rules": [],
                    "pricing": null
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "jina-reranker-v1-turbo-en"
                    },
                    "model": "jina-reranker-v1-turbo-en",
                    "model_properties": {
                        "context_size": 8192
                    },
                    "model_type": "rerank",
                    "parameter_rules": [],
                    "pricing": null
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "jina-reranker-v2-base-multilingual"
                    },
                    "model": "jina-reranker-v2-base-multilingual",
                    "model_properties": {
                        "context_size": 8192
                    },
                    "model_type": "rerank",
                    "parameter_rules": [],
                    "pricing": null
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "jina-clip-v1"
                    },
                    "model": "jina-clip-v1",
                    "model_properties": {
                        "context_size": 8192,
                        "max_chunks": 2048
                    },
                    "model_type": "text-embedding",
                    "parameter_rules": [],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.001",
                        "output": null,
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "jina-clip-v2"
                    },
                    "model": "jina-clip-v2",
                    "model_properties": {
                        "context_size": 8192,
                        "max_chunks": 2048
                    },
                    "model_type": "text-embedding",
                    "parameter_rules": [],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.001",
                        "output": null,
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "jina-embeddings-v2-base-de"
                    },
                    "model": "jina-embeddings-v2-base-de",
                    "model_properties": {
                        "context_size": 8192,
                        "max_chunks": 2048
                    },
                    "model_type": "text-embedding",
                    "parameter_rules": [],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.001",
                        "output": null,
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "jina-embeddings-v2-base-en"
                    },
                    "model": "jina-embeddings-v2-base-en",
                    "model_properties": {
                        "context_size": 8192,
                        "max_chunks": 2048
                    },
                    "model_type": "text-embedding",
                    "parameter_rules": [],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.001",
                        "output": null,
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "jina-embeddings-v2-base-zh"
                    },
                    "model": "jina-embeddings-v2-base-zh",
                    "model_properties": {
                        "context_size": 8192,
                        "max_chunks": 2048
                    },
                    "model_type": "text-embedding",
                    "parameter_rules": [],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.001",
                        "output": null,
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "jina-embeddings-v2-small-en"
                    },
                    "model": "jina-embeddings-v2-small-en",
                    "model_properties": {
                        "context_size": 8192,
                        "max_chunks": 2048
                    },
                    "model_type": "text-embedding",
                    "parameter_rules": [],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.001",
                        "output": null,
                        "unit": "0.001"
                    }
                },
                {
                    "deprecated": false,
                    "features": null,
                    "fetch_from": "predefined-model",
                    "label": {
                        "en_US": "jina-embeddings-v3"
                    },
                    "model": "jina-embeddings-v3",
                    "model_properties": {
                        "context_size": 8192,
                        "max_chunks": 2048
                    },
                    "model_type": "text-embedding",
                    "parameter_rules": [],
                    "pricing": {
                        "currency": "USD",
                        "input": "0.001",
                        "output": null,
                        "unit": "0.001"
                    }
                }
            ],
            "position": {
                "rerank": [
                    "jina-reranker-v2-base-multilingual",
                    "jina-reranker-v1-base-en",
                    "jina-reranker-v1-turbo-en",
                    "jina-colbert-v1-en",
                    "jina-reranker-v1-tiny-en"
                ],
                "text_embedding": null
            },
            "provider": "jina",
            "provider_credential_schema": {
                "credential_form_schemas": [
                    {
                        "default": null,
                        "label": {
                            "en_US": "API Key"
                        },
                        "max_length": 0,
                        "options": [],
                        "placeholder": {
                            "en_US": "Enter your API Key",
                            "zh_Hans": "在此输入您的 API Key"
                        },
                        "required": true,
                        "show_on": [],
                        "type": "secret-input",
                        "variable": "api_key"
                    }
                ]
            },
            "supported_model_types": [
                "text-embedding",
                "rerank"
            ]
        },
        "tool": {},
        "agent_strategy": {},
        "install_count": 4529,
        "version_updated_at": "2025-03-13T02:54:34.726061Z",
        "latest_version": "0.0.6",
        "latest_package_identifier": "langgenius/jina:0.0.6@3ac399c96509b3bfe3124f9cdfa13999cb2f7d7ff00a99cc8d3ac4f4d82ffe6c",
        "tags": []
    },
    {
        "type": "plugin",
        "name": "dalle",
        "org": "langgenius",
        "plugin_id": "langgenius/dalle",
        "icon": "langgenius/packages/dalle/_assets/icon.png",
        "label": {
            "en_US": "DALL-E",
            "pt_BR": "DALL-E",
            "zh_Hans": "DALL-E 绘画"
        },
        "brief": {
            "en_US": "DALL-E art",
            "pt_BR": "DALL-E art",
            "zh_Hans": "DALL-E 绘画"
        },
        "introduction": "# DALL-E\r\n\r\n## Overview\r\n\r\nDALL-E is an AI image generator developed by OpenAI that generates images based on text prompts. Dify has integrated the DALL-E tool, and the following are the steps to configure and use the DALL-E tool in Dify.\r\n\r\n## Configure\r\n\r\n### 1. Apply for Dall-E API Key\r\n\r\nPlease apply for an API Key on the [OpenAI Platform](https://platform.openai.com/).\r\n\r\n### 2. Get Dall-E tools from Plugin Marketplace\r\n\r\nThe Dall-E tools could be found at the Plugin Marketplace, please install it first.\r\n\r\n### 3. Fill in the configuration in Dify\r\n\r\nOn the Dify navigation page, click `Tools > DALL-E > Authorize` and fill in the API Key.\r\n\r\n**Note:** Base URL and Organization ID are optional. The Organization IDs can be found on your [Organization settings](https://platform.openai.com/settings/organization/general) page.\r\n\r\n![DALL-E Configuration](./_assets/dalle_1.PNG)\r\n\r\n### 4. Use the tool\r\n\r\nYou can use the Dall-E tool in the following application types:\r\n\r\n#### Chatflow / Workflow applications\r\n\r\n![Chatflow/Workflow Application](./_assets/dalle_2.PNG)\r\n\r\nBoth Chatflow and Workflow applications support the `DALL-E` tool node. After adding it, you need to fill in the \"Input Variables → Prompt\" in the node with variables to reference the user's input prompt or the content generated by the previous node. Finally, use the variable to reference the image output by `DALL-E` in the \"End\" node.\r\n\r\n#### Agent applications\r\n\r\n![Agent Application](./_assets/dalle_3.PNG)\r\n\r\nAdd the Dall-E tool in the Agent application, then send a picture description in the dialog box to call the tool to generate an AI image.\r\n",
        "category": "tool",
        "repository": "",
        "resource": {
            "memory": 1048576,
            "permission": {
                "model": {
                    "enabled": true,
                    "llm": true,
                    "moderation": false,
                    "rerank": false,
                    "speech2text": false,
                    "text_embedding": false,
                    "tts": false
                },
                "tool": {
                    "enabled": true
                }
            }
        },
        "privacy_options": "",
        "privacy_policy": "",
        "created_at": "2024-11-27T10:53:55.719457Z",
        "updated_at": "2025-02-17T07:13:15.2132Z",
        "badges": [],
        "plugins": {
            "agent_strategies": null,
            "endpoints": null,
            "models": null,
            "tools": [
                "provider/dalle.yaml"
            ]
        },
        "endpoint": {},
        "model": {},
        "tool": {
            "credentials_schema": [
                {
                    "default": null,
                    "helper": null,
                    "label": {
                        "en_US": "OpenAI API key",
                        "pt_BR": "OpenAI API key",
                        "zh_Hans": "OpenAI API key"
                    },
                    "name": "openai_api_key",
                    "options": null,
                    "placeholder": {
                        "en_US": "Please input your OpenAI API key",
                        "pt_BR": "Please input your OpenAI API key",
                        "zh_Hans": "请输入你的 OpenAI API key"
                    },
                    "required": true,
                    "scope": null,
                    "type": "secret-input",
                    "url": null
                },
                {
                    "default": null,
                    "helper": null,
                    "label": {
                        "en_US": "OpenAI base URL",
                        "pt_BR": "OpenAI base URL",
                        "zh_Hans": "OpenAI base URL"
                    },
                    "name": "openai_base_url",
                    "options": null,
                    "placeholder": {
                        "en_US": "Please input your OpenAI base URL",
                        "pt_BR": "Please input your OpenAI base URL",
                        "zh_Hans": "请输入你的 OpenAI base URL"
                    },
                    "required": false,
                    "scope": null,
                    "type": "text-input",
                    "url": null
                },
                {
                    "default": null,
                    "helper": null,
                    "label": {
                        "en_US": "OpenAI organization ID",
                        "pt_BR": "OpenAI organization ID",
                        "zh_Hans": "OpenAI organization ID"
                    },
                    "name": "openai_organization_id",
                    "options": null,
                    "placeholder": {
                        "en_US": "Please input your OpenAI organization ID",
                        "pt_BR": "Please input your OpenAI organization ID",
                        "zh_Hans": "请输入你的 OpenAI organization ID"
                    },
                    "required": false,
                    "scope": null,
                    "type": "text-input",
                    "url": null
                }
            ],
            "identity": {
                "author": "langgenius",
                "description": {
                    "en_US": "DALL-E art",
                    "pt_BR": "DALL-E art",
                    "zh_Hans": "DALL-E 绘画"
                },
                "icon": "icon.png",
                "label": {
                    "en_US": "DALL-E",
                    "pt_BR": "DALL-E",
                    "zh_Hans": "DALL-E 绘画"
                },
                "name": "dalle",
                "tags": [
                    "image",
                    "productivity"
                ]
            },
            "tools": [
                {
                    "description": {
                        "human": {
                            "en_US": "DALL-E is a text to image tool",
                            "pt_BR": "DALL-E is a text to image tool",
                            "zh_Hans": "DALL-E 是一个文本到图像的工具"
                        },
                        "llm": "DALL-E is a tool used to generate images from text"
                    },
                    "has_runtime_parameters": false,
                    "identity": {
                        "author": "langgenius",
                        "label": {
                            "en_US": "DALL-E 2",
                            "zh_Hans": "DALL-E 2 绘画"
                        },
                        "name": "dalle2"
                    },
                    "output_schema": null,
                    "parameters": [
                        {
                            "auto_generate": null,
                            "default": null,
                            "form": "llm",
                            "human_description": {
                                "en_US": "Image prompt, you can check the official documentation of DallE 2",
                                "pt_BR": "Image prompt, you can check the official documentation of DallE 2",
                                "zh_Hans": "图像提示词，您可以查看 DallE 2 的官方文档"
                            },
                            "label": {
                                "en_US": "Prompt",
                                "pt_BR": "Prompt",
                                "zh_Hans": "提示词"
                            },
                            "llm_description": "Image prompt of DallE 2, you should describe the image you want to generate as a list of words as possible as detailed",
                            "max": null,
                            "min": null,
                            "name": "prompt",
                            "options": null,
                            "precision": null,
                            "required": true,
                            "scope": null,
                            "template": null,
                            "type": "string"
                        },
                        {
                            "auto_generate": null,
                            "default": "large",
                            "form": "form",
                            "human_description": {
                                "en_US": "used for selecting the image size",
                                "pt_BR": "used for selecting the image size",
                                "zh_Hans": "用于选择图像大小"
                            },
                            "label": {
                                "en_US": "Image size",
                                "pt_BR": "Image size",
                                "zh_Hans": "图像大小"
                            },
                            "llm_description": "",
                            "max": null,
                            "min": null,
                            "name": "size",
                            "options": [
                                {
                                    "label": {
                                        "en_US": "Small(256x256)",
                                        "pt_BR": "Small(256x256)",
                                        "zh_Hans": "小(256x256)"
                                    },
                                    "value": "small"
                                },
                                {
                                    "label": {
                                        "en_US": "Medium(512x512)",
                                        "pt_BR": "Medium(512x512)",
                                        "zh_Hans": "中(512x512)"
                                    },
                                    "value": "medium"
                                },
                                {
                                    "label": {
                                        "en_US": "Large(1024x1024)",
                                        "pt_BR": "Large(1024x1024)",
                                        "zh_Hans": "大(1024x1024)"
                                    },
                                    "value": "large"
                                }
                            ],
                            "precision": null,
                            "required": true,
                            "scope": null,
                            "template": null,
                            "type": "select"
                        },
                        {
                            "auto_generate": null,
                            "default": 1,
                            "form": "form",
                            "human_description": {
                                "en_US": "used for selecting the number of images",
                                "pt_BR": "used for selecting the number of images",
                                "zh_Hans": "用于选择图像数量"
                            },
                            "label": {
                                "en_US": "Number of images",
                                "pt_BR": "Number of images",
                                "zh_Hans": "图像数量"
                            },
                            "llm_description": "",
                            "max": 10,
                            "min": 1,
                            "name": "n",
                            "options": null,
                            "precision": null,
                            "required": true,
                            "scope": null,
                            "template": null,
                            "type": "number"
                        }
                    ]
                },
                {
                    "description": {
                        "human": {
                            "en_US": "DALL-E is a text to image tool",
                            "pt_BR": "DALL-E is a text to image tool",
                            "zh_Hans": "DALL-E 是一个文本到图像的工具"
                        },
                        "llm": "DALL-E is a tool used to generate images from text"
                    },
                    "has_runtime_parameters": false,
                    "identity": {
                        "author": "langgenius",
                        "label": {
                            "en_US": "DALL-E 3",
                            "pt_BR": "DALL-E 3",
                            "zh_Hans": "DALL-E 3 绘画"
                        },
                        "name": "dalle3"
                    },
                    "output_schema": null,
                    "parameters": [
                        {
                            "auto_generate": null,
                            "default": null,
                            "form": "llm",
                            "human_description": {
                                "en_US": "Image prompt, you can check the official documentation of DallE 3",
                                "pt_BR": "Image prompt, you can check the official documentation of DallE 3",
                                "zh_Hans": "图像提示词，您可以查看 DallE 3 的官方文档"
                            },
                            "label": {
                                "en_US": "Prompt",
                                "pt_BR": "Prompt",
                                "zh_Hans": "提示词"
                            },
                            "llm_description": "Image prompt of DallE 3, you should describe the image you want to generate as a list of words as possible as detailed",
                            "max": null,
                            "min": null,
                            "name": "prompt",
                            "options": null,
                            "precision": null,
                            "required": true,
                            "scope": null,
                            "template": null,
                            "type": "string"
                        },
                        {
                            "auto_generate": null,
                            "default": "square",
                            "form": "form",
                            "human_description": {
                                "en_US": "selecting the image size",
                                "pt_BR": "selecting the image size",
                                "zh_Hans": "选择图像大小"
                            },
                            "label": {
                                "en_US": "Image size",
                                "pt_BR": "Image size",
                                "zh_Hans": "图像大小"
                            },
                            "llm_description": "",
                            "max": null,
                            "min": null,
                            "name": "size",
                            "options": [
                                {
                                    "label": {
                                        "en_US": "Squre(1024x1024)",
                                        "pt_BR": "Squre(1024x1024)",
                                        "zh_Hans": "方(1024x1024)"
                                    },
                                    "value": "square"
                                },
                                {
                                    "label": {
                                        "en_US": "Vertical(1024x1792)",
                                        "pt_BR": "Vertical(1024x1792)",
                                        "zh_Hans": "竖屏(1024x1792)"
                                    },
                                    "value": "vertical"
                                },
                                {
                                    "label": {
                                        "en_US": "Horizontal(1792x1024)",
                                        "pt_BR": "Horizontal(1792x1024)",
                                        "zh_Hans": "横屏(1792x1024)"
                                    },
                                    "value": "horizontal"
                                }
                            ],
                            "precision": null,
                            "required": true,
                            "scope": null,
                            "template": null,
                            "type": "select"
                        },
                        {
                            "auto_generate": null,
                            "default": 1,
                            "form": "form",
                            "human_description": {
                                "en_US": "selecting the number of images",
                                "pt_BR": "selecting the number of images",
                                "zh_Hans": "选择图像数量"
                            },
                            "label": {
                                "en_US": "Number of images",
                                "pt_BR": "Number of images",
                                "zh_Hans": "图像数量"
                            },
                            "llm_description": "",
                            "max": 1,
                            "min": 1,
                            "name": "n",
                            "options": null,
                            "precision": null,
                            "required": true,
                            "scope": null,
                            "template": null,
                            "type": "number"
                        },
                        {
                            "auto_generate": null,
                            "default": "standard",
                            "form": "form",
                            "human_description": {
                                "en_US": "selecting the image quality",
                                "pt_BR": "selecting the image quality",
                                "zh_Hans": "选择图像质量"
                            },
                            "label": {
                                "en_US": "Image quality",
                                "pt_BR": "Image quality",
                                "zh_Hans": "图像质量"
                            },
                            "llm_description": "",
                            "max": null,
                            "min": null,
                            "name": "quality",
                            "options": [
                                {
                                    "label": {
                                        "en_US": "Standard",
                                        "pt_BR": "Standard",
                                        "zh_Hans": "标准"
                                    },
                                    "value": "standard"
                                },
                                {
                                    "label": {
                                        "en_US": "HD",
                                        "pt_BR": "HD",
                                        "zh_Hans": "高清"
                                    },
                                    "value": "hd"
                                }
                            ],
                            "precision": null,
                            "required": true,
                            "scope": null,
                            "template": null,
                            "type": "select"
                        },
                        {
                            "auto_generate": null,
                            "default": "vivid",
                            "form": "form",
                            "human_description": {
                                "en_US": "selecting the image style",
                                "pt_BR": "selecting the image style",
                                "zh_Hans": "选择图像风格"
                            },
                            "label": {
                                "en_US": "Image style",
                                "pt_BR": "Image style",
                                "zh_Hans": "图像风格"
                            },
                            "llm_description": "",
                            "max": null,
                            "min": null,
                            "name": "style",
                            "options": [
                                {
                                    "label": {
                                        "en_US": "Vivid",
                                        "pt_BR": "Vivid",
                                        "zh_Hans": "生动"
                                    },
                                    "value": "vivid"
                                },
                                {
                                    "label": {
                                        "en_US": "Natural",
                                        "pt_BR": "Natural",
                                        "zh_Hans": "自然"
                                    },
                                    "value": "natural"
                                }
                            ],
                            "precision": null,
                            "required": true,
                            "scope": null,
                            "template": null,
                            "type": "select"
                        }
                    ]
                }
            ]
        },
        "agent_strategy": {},
        "install_count": 5790,
        "version_updated_at": "2025-02-17T07:13:15.213165Z",
        "latest_version": "0.0.4",
        "latest_package_identifier": "langgenius/dalle:0.0.4@c0d3625ef2fe13b8e8f6125fb4335822904d382e9c65e592f0d44dca037e3f6a",
        "tags": [
            {
                "name": "image"
            },
            {
                "name": "productivity"
            }
        ]
    },
    {
        "type": "plugin",
        "name": "github",
        "org": "langgenius",
        "plugin_id": "langgenius/github",
        "icon": "langgenius/packages/github/_assets/icon.svg",
        "label": {
            "en_US": "Github",
            "pt_BR": "Github",
            "zh_Hans": "Github"
        },
        "brief": {
            "en_US": "GitHub is an online software source code hosting service.",
            "pt_BR": "GitHub é uma plataforma online para serviços de hospedagem de código fonte de software.",
            "zh_Hans": "GitHub是一个在线软件源代码托管服务平台。"
        },
        "introduction": "# GitHub\n\n## Overview\n\nGitHub is a web-based platform for version control and collaboration, primarily used for software development. As a tool in Dify, it provides users the ability to search repositories by keywords.\n\n## Configuration\n\n### 1. Apply for API Key and Version\n\nPlease apply for an [API Key](https://github.com/settings/personal-access-tokens) and the [API version](https://docs.github.com/en/rest/about-the-rest-api/api-versions?apiVersion=2022-11-28).\n\n### 2. Get GitHub tools from Plugin Marketplace\n\nThe GitHub tools can be found in the Plugin Marketplace. Please install it first.\n\n### 3. Fill in the configuration in Dify\n\nOn the Dify navigation page, click `Tools > GitHub > Authorize` and fill in the API Key.\n\n![](./_assets/github_1.PNG)\n\n### 4. Using the tool\n\nYou can use the GitHub tool in the following application types:\n\n#### Chatflow / Workflow applications\n\nBoth Chatflow and Workflow applications support adding a GitHub tool node.\n\n![](./_assets/github_2.PNG)\n\n#### Agent applications\n\nAdd the GitHub tool in the Agent application, then enter repository search instructions to call this tool.\n\n![](./_assets/github_3.PNG)",
        "category": "tool",
        "repository": "",
        "resource": {
            "memory": 1048576,
            "permission": {
                "model": {
                    "enabled": true,
                    "llm": true,
                    "moderation": false,
                    "rerank": false,
                    "speech2text": false,
                    "text_embedding": false,
                    "tts": false
                },
                "tool": {
                    "enabled": true
                }
            }
        },
        "privacy_options": "",
        "privacy_policy": "",
        "created_at": "2024-11-29T06:16:42.366115Z",
        "updated_at": "2025-02-17T07:12:47.909233Z",
        "badges": [],
        "plugins": {
            "agent_strategies": null,
            "endpoints": null,
            "models": null,
            "tools": [
                "provider/github.yaml"
            ]
        },
        "endpoint": {},
        "model": {},
        "tool": {
            "credentials_schema": [
                {
                    "default": null,
                    "helper": null,
                    "label": {
                        "en_US": "Access Tokens",
                        "pt_BR": "Tokens de acesso",
                        "zh_Hans": "Access Tokens"
                    },
                    "name": "access_tokens",
                    "options": null,
                    "placeholder": {
                        "en_US": "Please input your Github Access Tokens",
                        "pt_BR": "Insira seus Tokens de Acesso do Github",
                        "zh_Hans": "请输入你的 Github Access Tokens"
                    },
                    "required": true,
                    "scope": null,
                    "type": "secret-input",
                    "url": "https://github.com/settings/tokens?type=beta"
                },
                {
                    "default": "2022-11-28",
                    "helper": null,
                    "label": {
                        "en_US": "API Version",
                        "pt_BR": "Versão da API",
                        "zh_Hans": "API Version"
                    },
                    "name": "api_version",
                    "options": null,
                    "placeholder": {
                        "en_US": "Please input your Github API Version",
                        "pt_BR": "Insira sua versão da API do Github",
                        "zh_Hans": "请输入你的 Github API Version"
                    },
                    "required": false,
                    "scope": null,
                    "type": "text-input",
                    "url": "https://docs.github.com/en/rest/about-the-rest-api/api-versions?apiVersion=2022-11-28"
                }
            ],
            "identity": {
                "author": "CharlieWei",
                "description": {
                    "en_US": "GitHub is an online software source code hosting service.",
                    "pt_BR": "GitHub é uma plataforma online para serviços de hospedagem de código fonte de software.",
                    "zh_Hans": "GitHub是一个在线软件源代码托管服务平台。"
                },
                "icon": "icon.svg",
                "label": {
                    "en_US": "Github",
                    "pt_BR": "Github",
                    "zh_Hans": "Github"
                },
                "name": "github",
                "tags": [
                    "utilities"
                ]
            },
            "tools": [
                {
                    "description": {
                        "human": {
                            "en_US": "Search the Github repository to retrieve the open source projects you need",
                            "pt_BR": "Pesquise o repositório do Github para recuperar os projetos de código aberto necessários.",
                            "zh_Hans": "搜索Github仓库，检索你需要的开源项目。"
                        },
                        "llm": "A tool when you wants to search for popular warehouses or open source projects for any keyword. format query condition like \"keywords+language:js\", language can be other dev languages."
                    },
                    "has_runtime_parameters": false,
                    "identity": {
                        "author": "CharlieWei",
                        "label": {
                            "en_US": "Search Repositories",
                            "pt_BR": "Pesquisar Repositórios",
                            "zh_Hans": "仓库搜索"
                        },
                        "name": "github_repositories"
                    },
                    "output_schema": null,
                    "parameters": [
                        {
                            "auto_generate": null,
                            "default": null,
                            "form": "llm",
                            "human_description": {
                                "en_US": "You want to find the project development language, keywords, For example. Find 10 Python developed PDF document parsing projects.",
                                "pt_BR": "Você deseja encontrar a linguagem de desenvolvimento do projeto, palavras-chave, Por exemplo. Encontre 10 projetos de análise de documentos PDF desenvolvidos em Python.",
                                "zh_Hans": "你想要找的项目开发语言、关键字，如：找10个Python开发的PDF文档解析项目。"
                            },
                            "label": {
                                "en_US": "query",
                                "pt_BR": "consulta",
                                "zh_Hans": "关键字"
                            },
                            "llm_description": "The query of you want to search, format query condition like \"keywords+language:js\", language can be other dev languages.",
                            "max": null,
                            "min": null,
                            "name": "query",
                            "options": null,
                            "precision": null,
                            "required": true,
                            "scope": null,
                            "template": null,
                            "type": "string"
                        },
                        {
                            "auto_generate": null,
                            "default": 5,
                            "form": "llm",
                            "human_description": {
                                "en_US": "Number of records returned by sorting based on stars. 5 is returned by default.",
                                "pt_BR": "Número de registros retornados por classificação com base em estrelas. 5 é retornado por padrão.",
                                "zh_Hans": "基于stars排序返回的记录数, 默认返回5条。"
                            },
                            "label": {
                                "en_US": "Top N",
                                "pt_BR": "Topo N",
                                "zh_Hans": "Top N"
                            },
                            "llm_description": "Extract the first N records from the returned result.",
                            "max": null,
                            "min": null,
                            "name": "top_n",
                            "options": null,
                            "precision": null,
                            "required": true,
                            "scope": null,
                            "template": null,
                            "type": "number"
                        }
                    ]
                }
            ]
        },
        "agent_strategy": {},
        "install_count": 3459,
        "version_updated_at": "2025-02-17T07:12:47.909197Z",
        "latest_version": "0.0.2",
        "latest_package_identifier": "langgenius/github:0.0.2@e65720a2d2f06233dd8e8b194d3b1b82e601aacf8c786ab4f958528981798226",
        "tags": [
            {
                "name": "utilities"
            }
        ]
    }
]